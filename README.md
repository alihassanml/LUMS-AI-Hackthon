# LUMS AI Hackathon: Magic Wand Project ğŸª„

Welcome to the **Magic Wand Project**, developed as part of the **LUMS AI Hackathon**! ğŸš€ Our innovative solution bridges the gap between gesture recognition and real-time AI-powered interaction, delivering a unique and engaging user experience.

---

## ğŸ¯ Project Overview

The Magic Wand project leverages **Streamlit** and **Groq API** to create an intuitive and interactive application for gesture-based controls. This AI-driven system can detect and classify hand gestures in real time, enabling seamless integration with various applications.

### Key Highlights:
- Utilized **LSTMs** and **GRUs** for handling sequential data and long-term dependencies.
- Achieved **89% accuracy** on Kaggle, securing **3rd place** on the leaderboard.
- Designed with an easy-to-use interface powered by **Streamlit** for rapid deployment.
- Integrated **Groq API** for high-performance model inference.

---

## ğŸš€ Features
- **Real-Time Gesture Recognition**: Classify hand gestures with precision.
- **Streamlined UI**: User-friendly interface for interaction and visualization.
- **Robust Model**: Trained on 100-frame sequences using advanced deep learning techniques.
- **Scalable Design**: Built for adaptability across various use cases.

---

## ğŸ› ï¸ Tech Stack
- **Frontend**: [Streamlit](https://streamlit.io/) for a responsive and interactive UI.
- **Backend**: Python with integration of the **Groq API** for high-performance model serving.
- **Deep Learning Framework**: TensorFlow/Keras for gesture recognition models.
- **Deployment**: Scalable architecture ready for deployment in production.

---

## ğŸ“ˆ Model Performance
Our gesture recognition model achieved the following:
- **Accuracy**: 89% on test data.
- **Leaderboard**: Secured **3rd place** on Kaggle during the LUMS AI Hackathon.

---

## ğŸ—ï¸ Installation

### Prerequisites
- Python 3.8 or higher
- Virtual Environment (optional but recommended)

### Steps
1. **Clone the Repository**:
   ```bash
   git clone https://github.com/alihassanml/LUMS-AI-Hackthon.git
   cd LUMS-AI-Hackthon
   ```

2. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the Application**:
   ```bash
   streamlit run app.py
   ```

4. **Access the Application**:
   - Open your browser and navigate to `http://localhost:8501`.

---

## ğŸ§  How It Works
1. **Data Collection**: Captures a sequence of 100 frames for each gesture.
2. **Model Training**: Utilizes LSTM and GRU networks to handle sequential data and classify gestures.
3. **Real-Time Prediction**: Inference pipeline powered by Groq API ensures fast and accurate predictions.

---

## ğŸ¤ Contributions
We welcome contributions! If you'd like to improve this project, feel free to:
1. Fork the repository.
2. Make your changes.
3. Submit a pull request.

---

## ğŸ“œ License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

---

## ğŸŒŸ Acknowledgments
Special thanks to the **LUMS AI Hackathon** organizers and my incredible team for their collaborative efforts.

For more details, contact [Ali Hassan](https://github.com/alihassanml).
